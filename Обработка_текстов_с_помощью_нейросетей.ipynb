{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Обработка текстов с помощью нейросетей",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMa+eUxXH/lGTHB5qq74Zy/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tlen33/Text-processing-with-neural-networks/blob/main/%D0%9E%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B0_%D1%82%D0%B5%D0%BA%D1%81%D1%82%D0%BE%D0%B2_%D1%81_%D0%BF%D0%BE%D0%BC%D0%BE%D1%89%D1%8C%D1%8E_%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D1%81%D0%B5%D1%82%D0%B5%D0%B9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bATIn_25PVBt"
      },
      "source": [
        "# Импорт библиотек\n",
        "\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Dense, Dropout, SpatialDropout1D, BatchNormalization, Embedding, Flatten, Activation\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "import time \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONttPPMJTlW0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49f46037-65cb-4ed5-e97b-53135624160d"
      },
      "source": [
        "# Подключение диска\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkXBquehVFyp"
      },
      "source": [
        "# Создаём функцию для чтения файла. На вход отправляем путь к файлу\n",
        "def readText(fileName):\n",
        "  f = open(fileName, 'r')\n",
        "  text = f.read()\n",
        "  text = text.replace(\"\\n\", \" \")\n",
        "  return text\n",
        "\n",
        "className = [\"О. Генри\", \"Стругацкие\", \"Булгаков\", \"Клиффорд\", \"Макс\", \"Брэдберри\"]\n",
        "nClasses = len(className)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0s-Y8qwVKAP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01bdf0c6-84e3-4634-de9e-bc78fd01520b"
      },
      "source": [
        "# Загружаем обучающие тексты\n",
        "trainText = []\n",
        "testText = []\n",
        "\n",
        "for i in className:\n",
        "  for j in os.listdir('/content/drive/MyDrive/Colab Notebooks/Базы/Тексты писателей/'):\n",
        "    if i in j:\n",
        "      if 'Обучающая' in j: \n",
        "        trainText.append(readText('/content/drive/MyDrive/Colab Notebooks/Базы/Тексты писателей/' + j))\n",
        "        print(j, 'добавлен в обучающую выборку')\n",
        "      if 'Тестовая' in j:\n",
        "        testText.append(readText('/content/drive/MyDrive/Colab Notebooks/Базы/Тексты писателей/' + j))\n",
        "        print(j, 'добавлен в тестовую выборку')\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(О. Генри) Тестовая_20 вместе.txt добавлен в тестовую выборку\n",
            "(О. Генри) Обучающая_50 вместе.txt добавлен в обучающую выборку\n",
            "\n",
            "(Стругацкие) Обучающая_5 вместе.txt добавлен в обучающую выборку\n",
            "(Стругацкие) Тестовая_2 вместе.txt добавлен в тестовую выборку\n",
            "\n",
            "(Булгаков) Обучающая_5 вместе.txt добавлен в обучающую выборку\n",
            "(Булгаков) Тестовая_2 вместе.txt добавлен в тестовую выборку\n",
            "\n",
            "(Клиффорд_Саймак) Тестовая_2 вместе.txt добавлен в тестовую выборку\n",
            "(Клиффорд_Саймак) Обучающая_5 вместе.txt добавлен в обучающую выборку\n",
            "\n",
            "(Макс Фрай) Тестовая_2 вместе.txt добавлен в тестовую выборку\n",
            "(Макс Фрай) Обучающая_5 вместе.txt добавлен в обучающую выборку\n",
            "\n",
            "(Рэй Брэдберри) Обучающая_22 вместе.txt добавлен в обучающую выборку\n",
            "(Рэй Брэдберри) Тестовая_8 вместе.txt добавлен в тестовую выборку\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tO-QQS3293gD",
        "outputId": "15d9cee1-d20c-4b7c-92ff-c537667f37a3"
      },
      "source": [
        "# Проверяем количество элементов и символов\n",
        "print(len(trainText))\n",
        "print(len(trainText[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "1049517\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwP71SrgVMKe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0895cf0f-ae43-4fc1-c2d7-a099cae69add"
      },
      "source": [
        "# Обработка данных\n",
        "cur_time = time.time()\n",
        "maxWordsCount = 20000\n",
        "tokenizer = Tokenizer(num_words=maxWordsCount, filters='!\"#$%&()*+,-–—./…:;<=>?@[\\\\]^_`{|}~«»\\t\\n\\xa0\\ufeff', lower=True, split=' ', oov_token='unknown', char_level=False)\n",
        "\n",
        "tokenizer.fit_on_texts(trainText)\n",
        "items = list(tokenizer.word_index.items())\n",
        "print('Время обработки: ', round(time.time() - cur_time, 2), 'c', sep='')\n",
        "print('50 самых часто встречающихся слов:\\n', items[:50])\n",
        "print(\"Размер словаря:\", len(items))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Время обработки: 3.74c\n",
            "50 самых часто встречающихся слов:\n",
            " [('unknown', 1), ('и', 2), ('в', 3), ('не', 4), ('я', 5), ('что', 6), ('на', 7), ('с', 8), ('он', 9), ('а', 10), ('как', 11), ('то', 12), ('это', 13), ('но', 14), ('все', 15), ('у', 16), ('по', 17), ('его', 18), ('к', 19), ('так', 20), ('мне', 21), ('из', 22), ('за', 23), ('меня', 24), ('ты', 25), ('же', 26), ('бы', 27), ('сказал', 28), ('вы', 29), ('было', 30), ('от', 31), ('они', 32), ('мы', 33), ('только', 34), ('да', 35), ('еще', 36), ('она', 37), ('о', 38), ('вот', 39), ('когда', 40), ('если', 41), ('уже', 42), ('был', 43), ('нет', 44), ('ни', 45), ('их', 46), ('ну', 47), ('чтобы', 48), ('до', 49), ('для', 50)]\n",
            "Размер словаря: 133070\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cSEExoqHWt5"
      },
      "source": [
        "# Преобразовываем текст в последовательность индексов согласно частотному словарю\n",
        "trainWordIndexes = tokenizer.texts_to_sequences(trainText)\n",
        "testWordIndexes = tokenizer.texts_to_sequences(testText)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdspxM_-WaP6"
      },
      "source": [
        "# Формирование обучающей выборки по листу индексов слов\n",
        "def getSetFromIndexes(wordIndexes, xLen, step):\n",
        "  xSample = []\n",
        "  wordsLen = len(wordIndexes)\n",
        "  index = 0\n",
        "\n",
        "  while (index + xLen <= wordsLen):\n",
        "    xSample.append(wordIndexes[index:index+xLen])\n",
        "    index += step\n",
        "\n",
        "  return xSample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uH8SP7iKXhFk"
      },
      "source": [
        "# Формирование обучающей и проверочной выборки\n",
        "def createSetsMultiClasses(wordIndexes, xLen, step):\n",
        "  nClasses = len(wordIndexes)\n",
        "  classesXSamples = []\n",
        "  for wI in wordIndexes:\n",
        "    classesXSamples.append(getSetFromIndexes(wI, xLen, step))\n",
        "\n",
        "  xSamples = []\n",
        "  ySamples = []\n",
        "  \n",
        "  for t in range(nClasses):\n",
        "    xT = classesXSamples[t]\n",
        "    for i in range(len(xT)):\n",
        "      xSamples.append(xT[i])\n",
        "      ySamples.append(utils.to_categorical(t, nClasses))\n",
        "\n",
        "  xSamples = np.array(xSamples)\n",
        "  ySamples = np.array(ySamples)\n",
        "\n",
        "  return (xSamples, ySamples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljuMFiuGXzaZ"
      },
      "source": [
        "# Формируем обучающую и тестовую выборку и преобразуем в матрицы\n",
        "xLen = 1000\n",
        "step = 100\n",
        "xTrain, yTrain = createSetsMultiClasses(trainWordIndexes, xLen, step)\n",
        "xTest, yTest = createSetsMultiClasses(testWordIndexes, xLen, step)\n",
        "xTrain01 = tokenizer.sequences_to_matrix(xTrain.tolist())\n",
        "xTest01 = tokenizer.sequences_to_matrix(xTest.tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tksb5FukUmqR"
      },
      "source": [
        "# Представляем тестовую выборку в удобных для распознавания размерах\n",
        "def createTestMultiClasses(wordIndexes, xLen, step):\n",
        "  nClasses = len(wordIndexes)\n",
        "  xTestClasses01 = []\n",
        "  xTestClasses = []\n",
        "  for wI in wordIndexes:\n",
        "    sample = (getSetFromIndexes(wI, xLen, step))\n",
        "    xTestClasses.append(sample)\n",
        "    xTestClasses01.append(tokenizer.sequences_to_matrix(sample))\n",
        "  xTestClasses01 = np.array(xTestClasses01)\n",
        "  xTestClasses = np.array(xTestClasses)\n",
        "  \n",
        "  return xTestClasses01, xTestClasses\n",
        "\n",
        "def recognizeMultiClass(model, xTest, modelName):\n",
        "  print(\"НЕЙРОНКА: \", modelName)\n",
        "  print()\n",
        "  \n",
        "  totalSumRec = 0\n",
        "  \n",
        "  for i in range(nClasses):\n",
        "    currPred = model.predict(xTest[i])\n",
        "    currOut = np.argmax(currPred, axis=1)\n",
        "    evVal = []\n",
        "    for j in range(nClasses):\n",
        "      evVal.append(len(currOut[currOut==j])/len(xTest[i]))\n",
        "    totalSumRec += len(currOut[currOut==i])\n",
        "    recognizedClass = np.argmax(evVal)\n",
        "    isRecognized = \"Это НЕПРАВИЛЬНЫЙ ответ!\"\n",
        "    if (recognizedClass == i):\n",
        "      isRecognized = \"Это ПРАВИЛЬНЫЙ ответ!\"\n",
        "    str1 = 'Класс: ' + className[i] + \" \" * (11 - len(className[i])) + str(int(100*evVal[i])) + \"% сеть отнесла к классу \" + className[recognizedClass]\n",
        "    print(str1, \" \" * (55-len(str1)), isRecognized, sep='')\n",
        "  \n",
        "  print()\n",
        "  sumCount = 0\n",
        "  for i in range(nClasses):\n",
        "    sumCount += len(xTest[i])\n",
        "  print(\"Средний процент распознавания: \", int(100*totalSumRec/sumCount), \"%\", sep='')\n",
        "  \n",
        "  return totalSumRec/sumCount"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtMzytr4aZv-",
        "outputId": "fe81aa59-6aea-49a5-ac51-1cf587e6b28e"
      },
      "source": [
        "xTestClasses01, x2 = createTestMultiClasses(testWordIndexes, xLen, step)\n",
        "l = np.array(xTestClasses01)\n",
        "np.save('xTestPredictBoW', l)\n",
        "np.save('xTestPredictEmbedding', x2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3RmJHqDarYK",
        "outputId": "6077ba54-3e91-4732-bced-85b6614cb720"
      },
      "source": [
        "# Создаём полносвязную сеть\n",
        "model = Sequential()\n",
        "model.add(Dense(200, input_dim=maxWordsCount, activation=\"relu\"))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(6, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(xTrain01, yTrain, epochs=10, batch_size=128, validation_data=(xTest01, yTest))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "138/138 [==============================] - 14s 94ms/step - loss: 0.0455 - accuracy: 0.9857 - val_loss: 0.4033 - val_accuracy: 0.8818\n",
            "Epoch 2/10\n",
            "138/138 [==============================] - 10s 75ms/step - loss: 3.1500e-04 - accuracy: 1.0000 - val_loss: 0.3504 - val_accuracy: 0.8841\n",
            "Epoch 3/10\n",
            "138/138 [==============================] - 10s 75ms/step - loss: 1.6298e-04 - accuracy: 1.0000 - val_loss: 0.3414 - val_accuracy: 0.8830\n",
            "Epoch 4/10\n",
            "138/138 [==============================] - 12s 84ms/step - loss: 1.0044e-04 - accuracy: 1.0000 - val_loss: 0.3410 - val_accuracy: 0.8803\n",
            "Epoch 5/10\n",
            "138/138 [==============================] - 10s 75ms/step - loss: 7.6769e-05 - accuracy: 1.0000 - val_loss: 0.3361 - val_accuracy: 0.8830\n",
            "Epoch 6/10\n",
            "138/138 [==============================] - 11s 78ms/step - loss: 5.6834e-05 - accuracy: 1.0000 - val_loss: 0.3344 - val_accuracy: 0.8824\n",
            "Epoch 7/10\n",
            "138/138 [==============================] - 10s 74ms/step - loss: 4.7518e-05 - accuracy: 1.0000 - val_loss: 0.3340 - val_accuracy: 0.8818\n",
            "Epoch 8/10\n",
            "138/138 [==============================] - 11s 78ms/step - loss: 3.8053e-05 - accuracy: 1.0000 - val_loss: 0.3339 - val_accuracy: 0.8820\n",
            "Epoch 9/10\n",
            "138/138 [==============================] - 11s 78ms/step - loss: 3.4797e-05 - accuracy: 1.0000 - val_loss: 0.3308 - val_accuracy: 0.8835\n",
            "Epoch 10/10\n",
            "138/138 [==============================] - 10s 74ms/step - loss: 2.7010e-05 - accuracy: 1.0000 - val_loss: 0.3302 - val_accuracy: 0.8830\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPzZkeKDew9l",
        "outputId": "40c0bdd6-a184-4fe9-f269-5f7e17ffb1ac"
      },
      "source": [
        "# Проверяем точность нейронки обученной на bag of words\n",
        "pred = recognizeMultiClass(model, xTestClasses01, \"нейросеть по классификации текстов писателей\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "НЕЙРОНКА:  нейросеть по классификации текстов писателей\n",
            "\n",
            "Класс: О. Генри   95% сеть отнесла к классу О. Генри   Это ПРАВИЛЬНЫЙ ответ!\n",
            "Класс: Стругацкие 87% сеть отнесла к классу Стругацкие Это ПРАВИЛЬНЫЙ ответ!\n",
            "Класс: Булгаков   71% сеть отнесла к классу Булгаков   Это ПРАВИЛЬНЫЙ ответ!\n",
            "Класс: Клиффорд   87% сеть отнесла к классу Клиффорд   Это ПРАВИЛЬНЫЙ ответ!\n",
            "Класс: Макс       91% сеть отнесла к классу Макс       Это ПРАВИЛЬНЫЙ ответ!\n",
            "Класс: Брэдберри  98% сеть отнесла к классу Брэдберри  Это ПРАВИЛЬНЫЙ ответ!\n",
            "\n",
            "Средний процент распознавания: 88%\n"
          ]
        }
      ]
    }
  ]
}